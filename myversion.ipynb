{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "docs = newsgroups.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章 1:\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "================================================================================\n",
      "文章 2:\n",
      "\n",
      "My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "\n",
      "================================================================================\n",
      "文章 3:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
      "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t*****\n",
      "\tIs't July in USA now????? Here in Sweden it's April and still cold.\n",
      "\tOr have you changed your calendar???\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t    ****************\n",
      "\t\t\t\t\t\t    ******************\n",
      "\t\t\t    ***************\n",
      "\n",
      "\n",
      "\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT's TRUE.\n",
      "\t\n",
      "\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\n",
      "\t\t\t\t\t\t    **************\n",
      "\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\n",
      "\t\n",
      "\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\n",
      "\tYOU FACIST!!!!!\n",
      "\n",
      "\n",
      "\n",
      "\tOhhh i forgot, this is how Armenians fight, nobody has forgot\n",
      "\tyou killings, rapings and torture against the Kurds and Turks once\n",
      "\tupon a time!\n",
      "      \n",
      "       \n",
      "\n",
      "\n",
      "Ohhhh so swedish RedCross workers do lie they too? What ever you say\n",
      "\"regional killer\", if you don't like the person then shoot him that's your policy.....l\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\tConfused?????\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "        Search Turkish planes? You don't know what you are talking about.\ti\n",
      "        Turkey's government has announced that it's giving weapons  <-----------i\n",
      "        to Azerbadjan since Armenia started to attack Azerbadjan\t\t\n",
      "        it self, not the Karabag province. So why search a plane for weapons\t\n",
      "        since it's content is announced to be weapons?   \n",
      "\n",
      "\tIf there is one that's confused then that's you! We have the right (and we do)\n",
      "\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\n",
      " \n",
      "\n",
      "\n",
      "\tShoot down with what? Armenian bread and butter? Or the arms and personel \n",
      "\tof the Russian army?\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 顯示前 3 篇文章的內容\n",
    "for i in range(3):\n",
    "    print(f\"文章 {i+1}:\\n\")\n",
    "    print(docs[i])\n",
    "    print(\"=\" * 80)  # 分隔線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前 3 篇文章的類別索引: [10  3 17]\n",
      "對應的新興論壇分類: ['rec.sport.hockey', 'comp.sys.ibm.pc.hardware', 'talk.politics.mideast']\n"
     ]
    }
   ],
   "source": [
    "# 顯示前 3 篇文章的類別索引\n",
    "print(\"前 3 篇文章的類別索引:\", newsgroups.target[:3])\n",
    "\n",
    "# 顯示對應的類別名稱\n",
    "print(\"對應的新興論壇分類:\", [newsgroups.target_names[idx] for idx in newsgroups.target[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r\"[^a-z\\s]\", \"\", doc)\n",
    "    tokens = doc.split()\n",
    "    return tokens\n",
    "\n",
    "docs_tokens = [preprocess(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'sure', 'some', 'bashers', 'of', 'pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'pens', 'massacre', 'of', 'the', 'devils', 'actually', 'i', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', 'however', 'i', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'nonpittsburghers', 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'pens', 'man', 'they', 'are', 'killing', 'those', 'devils', 'worse', 'than', 'i', 'thought', 'jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats', 'he', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs', 'bowman', 'should', 'let', 'jagr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'jersey', 'anyway', 'i', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'islanders', 'lose', 'the', 'final', 'regular', 'season', 'game', 'pens', 'rule']\n"
     ]
    }
   ],
   "source": [
    "print(docs_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立詞彙表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "詞彙表大小: 121462\n"
     ]
    }
   ],
   "source": [
    "# 建立詞彙表\n",
    "all_tokens = [token for doc in docs_tokens for token in doc]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "vocab = list(vocab_counter.keys())\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 建立 word2idx 和 idx2word 對應\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "\n",
    "print(f\"詞彙表大小: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建構skip-gram訓練數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正樣本數量: 12893026, 負樣本數量: 64465130\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_skipgram_pairs(docs_tokens, window_size=2, num_neg_samples=5):\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "    \n",
    "    for tokens in docs_tokens:\n",
    "        indices = [word2idx[word] for word in tokens if word in word2idx]\n",
    "        \n",
    "        for i, target in enumerate(indices):\n",
    "            window_start = max(i - window_size, 0)\n",
    "            window_end = min(i + window_size + 1, len(indices))\n",
    "            \n",
    "            # 取得 context 單詞\n",
    "            context_words = indices[window_start:i] + indices[i+1:window_end]\n",
    "            for ctx in context_words:\n",
    "                positive_pairs.append((target, ctx))\n",
    "                \n",
    "                # 生成負樣本\n",
    "                for _ in range(num_neg_samples):\n",
    "                    neg_word = random.randint(0, vocab_size - 1)\n",
    "                    negative_pairs.append((target, neg_word))\n",
    "    \n",
    "    return positive_pairs, negative_pairs\n",
    "\n",
    "# 產生 skip-gram 訓練數據\n",
    "positive_pairs, negative_pairs = generate_skipgram_pairs(docs_tokens, window_size=2, num_neg_samples=5)\n",
    "print(f\"正樣本數量: {len(positive_pairs)}, 負樣本數量: {len(negative_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉換成 PyTorch 張量\n",
    "positive_pairs_tensor = torch.tensor(positive_pairs, dtype=torch.long)\n",
    "negative_pairs_tensor = torch.tensor(negative_pairs, dtype=torch.long)\n",
    "\n",
    "# 使用 TensorDataset 和 DataLoader\n",
    "batch_size = 1024\n",
    "# 正樣本 DataLoader\n",
    "positive_dataset = TensorDataset(positive_pairs_tensor)\n",
    "positive_data_loader = DataLoader(positive_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 負樣本 DataLoader\n",
    "negative_dataset = TensorDataset(negative_pairs_tensor)\n",
    "negative_data_loader = DataLoader(negative_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義 Word2Vec Skip-Gram模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, center_word, context_word):\n",
    "        v_pvt = self.embedding(center_word)  # 主要單詞向量\n",
    "        v_ctx = self.embedding(context_word)  # 背景單詞向量\n",
    "        return torch.sum(v_pvt * v_ctx, dim=1)  # 內積計算相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義損失函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling_loss(pos_scores, neg_scores):\n",
    "    \"\"\"\n",
    "    pos_scores: 正樣本的點積結果\n",
    "    neg_scores: 負樣本的點積結果\n",
    "    \"\"\"\n",
    "    pos_loss = -F.logsigmoid(pos_scores).mean()  # 第一項\n",
    "    neg_loss = -F.logsigmoid(-neg_scores).mean()  # 第二項\n",
    "    return pos_loss + neg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 20232.0747\n",
      "Epoch 2/5, Loss: 14660.2711\n",
      "Epoch 3/5, Loss: 14233.7238\n",
      "Epoch 4/5, Loss: 14030.1474\n",
      "Epoch 5/5, Loss: 13931.2454\n",
      "訓練完成！\n"
     ]
    }
   ],
   "source": [
    "# 設定超參數\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SkipGramModel(vocab_size, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 訓練迴圈\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # 使用 zip() 同時迭代兩個 DataLoader\n",
    "    for (pos_batch,), (neg_batch,) in zip(positive_data_loader, negative_data_loader):\n",
    "        pos_batch, neg_batch = pos_batch.to(device), neg_batch.to(device)\n",
    "        center_word_pos, context_word_pos = pos_batch[:, 0], pos_batch[:, 1]\n",
    "        center_word_neg, context_word_neg = neg_batch[:, 0], neg_batch[:, 1]\n",
    "\n",
    "        # 正樣本分數\n",
    "        pos_scores = model(center_word_pos, context_word_pos)\n",
    "\n",
    "        # 負樣本分數\n",
    "        neg_scores = model(center_word_neg, context_word_neg)\n",
    "\n",
    "        # 計算損失\n",
    "        loss = negative_sampling_loss(pos_scores, neg_scores)\n",
    "\n",
    "        # 反向傳播與優化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"訓練完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試嵌入向量效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tibetian: [-1.4375777  -0.68293995  2.1688416   0.4099418   0.43794662] ...\n",
      "psyhtjsmipscccnottinghamacuk: [-0.4609     -0.74916387 -0.06740743 -2.334864   -1.9351561 ] ...\n",
      "separatist: [-1.0169411  -2.9384568  -0.52474445 -1.0964781  -0.19436616] ...\n",
      "academics: [-1.014023    0.35530466  0.8775246   0.08616684  0.8812692 ] ...\n",
      "mjglzprscqdgaqvpgtitcvcnnj: [-0.25967306  0.90539557  0.7625903  -1.0720445   0.6960263 ] ...\n",
      "blank: [ 0.1894634  -0.39105284  0.04489758 -0.11747424  0.15941003] ...\n",
      "repainted: [ 0.5745938  -1.3974309  -0.7704798  -0.08036944 -1.07997   ] ...\n",
      "cbts: [-3.7467105   1.158229   -2.5395732   0.55880266  0.28207058] ...\n",
      "futility: [ 2.3441868  1.1798515  3.284471  -1.591921   0.4799493] ...\n",
      "qatar: [-0.819624  -1.1575304  3.0840986 -1.0137874  1.5982919] ...\n"
     ]
    }
   ],
   "source": [
    "# 取得單詞的嵌入向量\n",
    "word_embeddings = model.embedding.weight.data.cpu().numpy()\n",
    "\n",
    "# 隨機選 10 個單詞，顯示其對應的嵌入向量\n",
    "sample_words = random.sample(vocab, 10)\n",
    "for word in sample_words:\n",
    "    idx = word2idx[word]\n",
    "    print(f\"{word}: {word_embeddings[idx][:5]} ...\")  # 顯示前 5 個數值"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
